{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem statement\n",
    "You have to write a python script which can fetch all the tweets(as many as allowed by Twitter API) done by midas@IIITD twitter handle and dump the responses into JSONlines file. The other part of your script should be able to parse these JSONline files to display the following for every tweet in a tabular format.\n",
    "\n",
    "The text of the tweet.\n",
    "Date and time of the tweet.\n",
    "The number of favorites/likes.\n",
    "The number of retweets.\n",
    "Number of Images present in Tweet. If no image returns None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = r'../Python_problem/tweets.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCESS_TOKEN = \"#\"\n",
    "ACCESS_TOKEN_SECRET = \"#\"\n",
    "CONSUMER_KEY = \"#\"\n",
    "CONSUMER_SECRET_KEY = \"#\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET_KEY)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_and_dump_tweets(screen_name):\n",
    "    '''\n",
    "    Parameters-\n",
    "        screen_name: Twitter handle of the user\n",
    "    Variables -\n",
    "        tweets: List (of length upto 200) of tweets by screen_name\n",
    "        oldest: Index of oldest fetched tweet\n",
    "        all_tweets_json: List of _json data from all_tweets\n",
    "    Returns -   \n",
    "        all_tweets: List of all tweets by screen_name\n",
    "    '''\n",
    "    #all_tweets intialized as an empty list\n",
    "    all_tweets = []\n",
    "    \n",
    "    #most recent tweets upto maximum of 200\n",
    "    tweets = api.user_timeline(screen_name = screen_name,count=200)\n",
    "    \n",
    "    #add tweets to all_tweets\n",
    "    all_tweets.extend(tweets)\n",
    "    \n",
    "    #ID of the most recent fetched tweet - 1\n",
    "    oldest = all_tweets[-1].id - 1\n",
    "    \n",
    "    #fetch tweets that are older than most recent 200 tweets until no tweet is left or 3200 limit is reached\n",
    "    while len(tweets) > 0:\n",
    "        \n",
    "        tweets = api.user_timeline(screen_name = screen_name, count=200, max_id=oldest)\n",
    "        all_tweets.extend(tweets)\n",
    "        oldest = all_tweets[-1].id - 1\n",
    "\n",
    "    print(f\"Total number of tweets from {screen_name} are {len(all_tweets)}\")\n",
    "    \n",
    "    #all_tweets_json initialized as an empty list\n",
    "    all_tweets_json = []\n",
    "    \n",
    "    #append _json corresponding to each of the tweet to the all_tweets_json\n",
    "    for tweet in all_tweets:\n",
    "        all_tweets_json.append(tweet._json)\n",
    "        \n",
    "    #dumping all_tweets_json to the file specified by FILE_PATH\n",
    "    #sort_keys is set to True for sorting dictionaries by key\n",
    "    #indent = 4, for pretty printing JSON array elements with indent level of 4\n",
    "    with open(FILE_PATH, 'w', encoding='utf8') as f:\n",
    "        json.dump(all_tweets_json, f, sort_keys = True,indent = 4)\n",
    "    return all_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_by_MIDAS = fetch_and_dump_tweets(\"midasIIITD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
